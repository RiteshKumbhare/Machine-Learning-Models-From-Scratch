{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7LZAroxf1qH",
        "outputId": "d01eb645-f1e3-4bce-9283-bef411204010"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Training set: (2000, 785), Labels: (2000, 1)\n",
            "Test set: (500, 785), Labels: (500, 1)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to [0,1]\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Flatten images: (28, 28) → (784,)\n",
        "x_train_flat = x_train.reshape(x_train.shape[0], -1)\n",
        "x_test_flat = x_test.reshape(x_test.shape[0], -1)\n",
        "\n",
        "# Add bias term: Append a column of ones (Now shape: (N, 785))\n",
        "x_train_bias = np.hstack([x_train_flat[:2000], np.ones((2000, 1))])\n",
        "x_test_bias = np.hstack([x_test_flat[:500], np.ones((500, 1))])\n",
        "\n",
        "# Select corresponding labels & reshape to column vectors\n",
        "y_train_sampled = y_train[:2000].reshape(-1, 1)\n",
        "y_test_sampled = y_test[:500].reshape(-1, 1)\n",
        "\n",
        "# Print final dataset shapes\n",
        "print(f\"Training set: {x_train_bias.shape}, Labels: {y_train_sampled.shape}\")\n",
        "print(f\"Test set: {x_test_bias.shape}, Labels: {y_test_sampled.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W = np.random.randn(785, 10) * 0.001\n",
        "num_classes = W.shape[1]"
      ],
      "metadata": {
        "id": "pXj_Sp9Af9t-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_vectorized(X, y, W, reg):\n",
        "  num_train = X.shape[0]\n",
        "  num_classes = W.shape[1]\n",
        "\n",
        "  scores = np.dot(X, W)\n",
        "  scores -= np.max(scores, axis=1, keepdims=True)\n",
        "\n",
        "  exp_scores = np.exp(scores)\n",
        "  softmax_probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "  correct_class_probs = softmax_probs[np.arange(num_train), y.flatten()]\n",
        "  loss = - np.sum(np.log(correct_class_probs)) / num_train\n",
        "  loss += reg * np.sum(W * W)\n",
        "\n",
        "  softmax_probs[np.arange(num_train), y.flatten()] -= 1\n",
        "  dW = np.dot(X.T, softmax_probs) / num_train\n",
        "  dW += 2 * reg * W\n",
        "\n",
        "  return loss, dW"
      ],
      "metadata": {
        "id": "XMFYOW7cgJWh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_train_vectorized(X, y, W, reg, alpha=0.01, iters=100):\n",
        "  num_train = X.shape[0]\n",
        "  for i in range(iters):\n",
        "    loss, dW = softmax_vectorized(X, y, W, reg)\n",
        "    W -= alpha * dW\n",
        "\n",
        "    if i % 10 == 0:\n",
        "      print(f\"Epoch {i}: Loss = {float(loss):.4f}\")\n",
        "  return W"
      ],
      "metadata": {
        "id": "IeAipaqyiawC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W = softmax_train_vectorized(x_train_bias, y_train_sampled, W, reg=0.0001, alpha=0.01, iters=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_VyFLgOie-l",
        "outputId": "ce82b6db-b63f-49a0-8f5b-8147047b86fc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss = 2.3032\n",
            "Epoch 10: Loss = 2.1968\n",
            "Epoch 20: Loss = 2.0985\n",
            "Epoch 30: Loss = 2.0074\n",
            "Epoch 40: Loss = 1.9228\n",
            "Epoch 50: Loss = 1.8444\n",
            "Epoch 60: Loss = 1.7717\n",
            "Epoch 70: Loss = 1.7044\n",
            "Epoch 80: Loss = 1.6421\n",
            "Epoch 90: Loss = 1.5843\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.dot(x_test_bias, W)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "accuracy = np.mean(y_pred == y_test_sampled)"
      ],
      "metadata": {
        "id": "HGxKCe7gijFX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gudt7gMHiqlF",
        "outputId": "4def9cef-72ea-483d-9beb-d0ae5a895f73"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.102644)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wcxhxNVAisfW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}